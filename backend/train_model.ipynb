{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and exploring data...\n",
      "Dataset shape: (3000, 18)\n",
      "\n",
      "Data types:\n",
      "date                 datetime64[ns]\n",
      "region                       object\n",
      "rainfall_24h                float64\n",
      "rainfall_72h                float64\n",
      "river_level                 float64\n",
      "soil_moisture               float64\n",
      "slope_gradient              float64\n",
      "vegetation_cover            float64\n",
      "urbanization                float64\n",
      "temperature                 float64\n",
      "snowmelt                    float64\n",
      "reservoir_level             float64\n",
      "previous_floods               int64\n",
      "flood_occurred                int64\n",
      "year                          int64\n",
      "month                         int64\n",
      "day                           int64\n",
      "flood_probability           float64\n",
      "dtype: object\n",
      "\n",
      "Check for missing values:\n",
      "date                 0\n",
      "region               0\n",
      "rainfall_24h         0\n",
      "rainfall_72h         0\n",
      "river_level          0\n",
      "soil_moisture        0\n",
      "slope_gradient       0\n",
      "vegetation_cover     0\n",
      "urbanization         0\n",
      "temperature          0\n",
      "snowmelt             0\n",
      "reservoir_level      0\n",
      "previous_floods      0\n",
      "flood_occurred       0\n",
      "year                 0\n",
      "month                0\n",
      "day                  0\n",
      "flood_probability    0\n",
      "dtype: int64\n",
      "\n",
      "Preprocessing data...\n",
      "\n",
      "Building models...\n",
      "\n",
      "Evaluating models...\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest Cross-Validation ROC-AUC: 0.9459 ± 0.0112\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92       326\n",
      "           1       0.93      0.86      0.89       274\n",
      "\n",
      "    accuracy                           0.91       600\n",
      "   macro avg       0.91      0.90      0.90       600\n",
      "weighted avg       0.91      0.91      0.90       600\n",
      "\n",
      "\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting Cross-Validation ROC-AUC: 0.9515 ± 0.0088\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       326\n",
      "           1       0.91      0.86      0.89       274\n",
      "\n",
      "    accuracy                           0.90       600\n",
      "   macro avg       0.90      0.90      0.90       600\n",
      "weighted avg       0.90      0.90      0.90       600\n",
      "\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anant\\hck\\cld\\my_env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:57:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\anant\\hck\\cld\\my_env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:57:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\anant\\hck\\cld\\my_env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:57:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\anant\\hck\\cld\\my_env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:57:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\anant\\hck\\cld\\my_env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:57:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\anant\\hck\\cld\\my_env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:57:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Cross-Validation ROC-AUC: 0.9455 ± 0.0096\n",
      "\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91       326\n",
      "           1       0.93      0.85      0.89       274\n",
      "\n",
      "    accuracy                           0.90       600\n",
      "   macro avg       0.91      0.90      0.90       600\n",
      "weighted avg       0.90      0.90      0.90       600\n",
      "\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Cross-Validation ROC-AUC: 0.9396 ± 0.0082\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       326\n",
      "           1       0.90      0.85      0.87       274\n",
      "\n",
      "    accuracy                           0.89       600\n",
      "   macro avg       0.89      0.88      0.89       600\n",
      "weighted avg       0.89      0.89      0.89       600\n",
      "\n",
      "\n",
      "Best model: Gradient Boosting with ROC-AUC: 0.9588\n",
      "\n",
      "Fine-tuning Gradient Boosting...\n",
      "Best parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n",
      "Best cross-validation score: 0.9515\n",
      "\n",
      "Tuned Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       326\n",
      "           1       0.91      0.86      0.89       274\n",
      "\n",
      "    accuracy                           0.90       600\n",
      "   macro avg       0.90      0.90      0.90       600\n",
      "weighted avg       0.90      0.90      0.90       600\n",
      "\n",
      "\n",
      "Top 10 features:\n",
      "rainfall_72h: 0.6261\n",
      "rainfall_24h: 0.1308\n",
      "urbanization: 0.0705\n",
      "vegetation_cover: 0.0334\n",
      "river_level: 0.0291\n",
      "previous_floods: 0.0289\n",
      "snowmelt: 0.0227\n",
      "slope_gradient: 0.0172\n",
      "reservoir_level: 0.0159\n",
      "soil_moisture: 0.0125\n",
      "\n",
      "Example prediction:\n",
      "Flood probability: 0.99\n",
      "Prediction: Flood\n",
      "Risk level: High\n",
      "\n",
      "Flood prediction model successfully built and saved as 'flood_prediction_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from xgboost import XGBClassifier\n",
    "# import joblib\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset_path = \"flood_prediction_dataset.csv\"\n",
    "# data = pd.read_csv(dataset_path)\n",
    "\n",
    "# # Convert date to datetime\n",
    "# data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# # Exploratory Data Analysis\n",
    "# def perform_eda(data):\n",
    "#     print(\"Dataset shape:\", data.shape)\n",
    "#     print(\"\\nData types:\")\n",
    "#     print(data.dtypes)\n",
    "    \n",
    "#     print(\"\\nCheck for missing values:\")\n",
    "#     print(data.isnull().sum())\n",
    "    \n",
    "#     # Correlation matrix for numerical features\n",
    "#     plt.figure(figsize=(14, 10))\n",
    "#     numeric_data = data.select_dtypes(include=[np.number])\n",
    "#     correlation = numeric_data.corr()\n",
    "#     sns.heatmap(correlation, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "#     plt.title(\"Correlation Matrix\")\n",
    "#     plt.savefig(\"correlation_matrix.png\")\n",
    "#     plt.close()\n",
    "    \n",
    "#     # Distribution of flood events by region\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     flood_by_region = data.groupby('region')['flood_occurred'].mean() * 100\n",
    "#     flood_by_region.sort_values().plot(kind='barh')\n",
    "#     plt.title(\"Percentage of Flood Events by Region\")\n",
    "#     plt.xlabel(\"Flood Occurrence (%)\")\n",
    "#     plt.savefig(\"flood_by_region.png\")\n",
    "#     plt.close()\n",
    "    \n",
    "#     # Rainfall vs Flood Occurrence\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     sns.boxplot(x='flood_occurred', y='rainfall_72h', data=data)\n",
    "#     plt.title(\"72-Hour Rainfall vs Flood Occurrence\")\n",
    "#     plt.savefig(\"rainfall_vs_flood.png\")\n",
    "#     plt.close()\n",
    "    \n",
    "#     # Seasonal patterns\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     monthly_floods = data.groupby('month')['flood_occurred'].mean() * 100\n",
    "#     monthly_floods.plot(kind='bar')\n",
    "#     plt.title(\"Monthly Flood Occurrence Percentage\")\n",
    "#     plt.xlabel(\"Month\")\n",
    "#     plt.ylabel(\"Flood Occurrence (%)\")\n",
    "#     plt.savefig(\"monthly_floods.png\")\n",
    "#     plt.close()\n",
    "    \n",
    "#     # Feature distributions based on flood occurrence\n",
    "#     important_features = ['rainfall_24h', 'rainfall_72h', 'river_level', 'soil_moisture', 'reservoir_level']\n",
    "#     fig, axes = plt.subplots(len(important_features), 1, figsize=(12, 15))\n",
    "    \n",
    "#     for i, feature in enumerate(important_features):\n",
    "#         sns.kdeplot(data=data, x=feature, hue='flood_occurred', ax=axes[i])\n",
    "#         axes[i].set_title(f'{feature} Distribution by Flood Occurrence')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"feature_distributions.png\")\n",
    "#     plt.close()\n",
    "\n",
    "# # Preprocess data\n",
    "# def preprocess_data(data):\n",
    "#     # Separate features and target\n",
    "#     X = data.drop(['flood_occurred', 'flood_probability', 'date'], axis=1)\n",
    "#     y = data['flood_occurred']\n",
    "    \n",
    "#     # Split categorical and numerical columns\n",
    "#     categorical_cols = ['region']\n",
    "#     numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "    \n",
    "#     # Create preprocessing steps\n",
    "#     numerical_transformer = StandardScaler()\n",
    "#     categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "#     # Create column transformer\n",
    "#     preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('num', numerical_transformer, numerical_cols),\n",
    "#             ('cat', categorical_transformer, categorical_cols)\n",
    "#         ])\n",
    "    \n",
    "#     # Split the data\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "#     return X_train, X_test, y_train, y_test, preprocessor, numerical_cols, categorical_cols\n",
    "\n",
    "# def build_models(X_train, y_train, preprocessor):\n",
    "#     # Model 1: Random Forest\n",
    "#     rf_model = Pipeline([\n",
    "#         ('preprocessor', preprocessor),\n",
    "#         ('classifier', RandomForestClassifier(random_state=42))\n",
    "#     ])\n",
    "    \n",
    "#     # Model 2: Gradient Boosting\n",
    "#     gb_model = Pipeline([\n",
    "#         ('preprocessor', preprocessor),\n",
    "#         ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "#     ])\n",
    "    \n",
    "#     # Model 3: XGBoost\n",
    "#     xgb_model = Pipeline([\n",
    "#         ('preprocessor', preprocessor),\n",
    "#         ('classifier', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "#     ])\n",
    "    \n",
    "#     # Model 4: Logistic Regression (baseline)\n",
    "#     lr_model = Pipeline([\n",
    "#         ('preprocessor', preprocessor),\n",
    "#         ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "#     ])\n",
    "    \n",
    "#     return {\n",
    "#         'Random Forest': rf_model,\n",
    "#         'Gradient Boosting': gb_model,\n",
    "#         'XGBoost': xgb_model,\n",
    "#         'Logistic Regression': lr_model\n",
    "#     }\n",
    "\n",
    "# def evaluate_models(models, X_train, y_train, X_test, y_test):\n",
    "#     results = {}\n",
    "    \n",
    "#     for name, model in models.items():\n",
    "#         print(f\"\\nTraining {name}...\")\n",
    "#         model.fit(X_train, y_train)\n",
    "        \n",
    "#         # Cross-validation\n",
    "#         cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "#         print(f\"{name} Cross-Validation ROC-AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        \n",
    "#         # Predictions on test set\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "#         # Classification report\n",
    "#         print(f\"\\n{name} Classification Report:\")\n",
    "#         print(classification_report(y_test, y_pred))\n",
    "        \n",
    "#         # Confusion matrix\n",
    "#         cm = confusion_matrix(y_test, y_pred)\n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "#         plt.title(f'{name} Confusion Matrix')\n",
    "#         plt.ylabel('True Label')\n",
    "#         plt.xlabel('Predicted Label')\n",
    "#         plt.savefig(f\"{name.replace(' ', '_').lower()}_confusion_matrix.png\")\n",
    "#         plt.close()\n",
    "        \n",
    "#         # ROC curve\n",
    "#         fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "#         roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "#         plt.plot([0, 1], [0, 1], 'k--')\n",
    "#         plt.xlim([0.0, 1.0])\n",
    "#         plt.ylim([0.0, 1.05])\n",
    "#         plt.xlabel('False Positive Rate')\n",
    "#         plt.ylabel('True Positive Rate')\n",
    "#         plt.title(f'{name} ROC Curve')\n",
    "#         plt.legend(loc=\"lower right\")\n",
    "#         plt.savefig(f\"{name.replace(' ', '_').lower()}_roc_curve.png\")\n",
    "#         plt.close()\n",
    "        \n",
    "#         # Precision-Recall curve\n",
    "#         precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "#         pr_auc = auc(recall, precision)\n",
    "        \n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         plt.plot(recall, precision, label=f'PR curve (area = {pr_auc:.3f})')\n",
    "#         plt.xlabel('Recall')\n",
    "#         plt.ylabel('Precision')\n",
    "#         plt.title(f'{name} Precision-Recall Curve')\n",
    "#         plt.legend(loc=\"lower left\")\n",
    "#         plt.savefig(f\"{name.replace(' ', '_').lower()}_pr_curve.png\")\n",
    "#         plt.close()\n",
    "        \n",
    "#         results[name] = {\n",
    "#             'model': model,\n",
    "#             'cv_score': cv_scores.mean(),\n",
    "#             'roc_auc': roc_auc,\n",
    "#             'pr_auc': pr_auc\n",
    "#         }\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def fine_tune_best_model(best_model_name, models, X_train, y_train, X_test, y_test):\n",
    "#     print(f\"\\nFine-tuning {best_model_name}...\")\n",
    "    \n",
    "#     if best_model_name == 'Random Forest':\n",
    "#         param_grid = {\n",
    "#             'classifier__n_estimators': [100, 200, 300],\n",
    "#             'classifier__max_depth': [None, 10, 20, 30],\n",
    "#             'classifier__min_samples_split': [2, 5, 10],\n",
    "#             'classifier__min_samples_leaf': [1, 2, 4]\n",
    "#         }\n",
    "#     elif best_model_name == 'Gradient Boosting':\n",
    "#         param_grid = {\n",
    "#             'classifier__n_estimators': [100, 200, 300],\n",
    "#             'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "#             'classifier__max_depth': [3, 5, 7]\n",
    "#         }\n",
    "#     elif best_model_name == 'XGBoost':\n",
    "#         param_grid = {\n",
    "#             'classifier__n_estimators': [100, 200, 300],\n",
    "#             'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "#             'classifier__max_depth': [3, 5, 7],\n",
    "#             'classifier__subsample': [0.8, 0.9, 1.0]\n",
    "#         }\n",
    "#     else:  # Logistic Regression\n",
    "#         param_grid = {\n",
    "#             'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "#             'classifier__penalty': ['l2'],\n",
    "#             'classifier__solver': ['lbfgs', 'liblinear']\n",
    "#         }\n",
    "    \n",
    "#     grid_search = GridSearchCV(\n",
    "#         models[best_model_name],\n",
    "#         param_grid,\n",
    "#         cv=5,\n",
    "#         scoring='roc_auc',\n",
    "#         n_jobs=-1\n",
    "#     )\n",
    "    \n",
    "#     grid_search.fit(X_train, y_train)\n",
    "    \n",
    "#     print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "#     print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "#     # Evaluate the tuned model\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "#     print(\"\\nTuned Model Classification Report:\")\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "    \n",
    "#     # Save the tuned model\n",
    "#     joblib.dump(best_model, \"flood_prediction_model.pkl\")\n",
    "    \n",
    "#     return best_model\n",
    "\n",
    "# def feature_importance(best_model, numerical_cols, categorical_cols):\n",
    "#     if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "#         # Get feature names from the preprocessor\n",
    "#         preprocessor = best_model.named_steps['preprocessor']\n",
    "#         feature_names = []\n",
    "        \n",
    "#         # Add numerical feature names\n",
    "#         feature_names.extend(numerical_cols)\n",
    "        \n",
    "#         # Add one-hot encoded categorical feature names\n",
    "#         onehot_features = []\n",
    "#         for cat in categorical_cols:\n",
    "#             encoder = preprocessor.named_transformers_['cat']\n",
    "#             categories = encoder.categories_[0]\n",
    "#             onehot_features.extend([f\"{cat}_{c}\" for c in categories])\n",
    "        \n",
    "#         feature_names.extend(onehot_features)\n",
    "        \n",
    "#         # Get feature importances\n",
    "#         importances = best_model.named_steps['classifier'].feature_importances_\n",
    "        \n",
    "#         # If the lengths don't match, adjust feature names\n",
    "#         if len(importances) != len(feature_names):\n",
    "#             feature_names = [f\"feature_{i}\" for i in range(len(importances))]\n",
    "        \n",
    "#         # Sort feature importances\n",
    "#         indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "#         # Plot feature importances\n",
    "#         plt.figure(figsize=(12, 8))\n",
    "#         plt.title('Feature Importances')\n",
    "#         plt.bar(range(len(indices)), importances[indices], align='center')\n",
    "#         plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(\"feature_importances.png\")\n",
    "#         plt.close()\n",
    "        \n",
    "#         # Return top 10 features\n",
    "#         top_features = [(feature_names[i], importances[i]) for i in indices[:10]]\n",
    "#         return top_features\n",
    "#     else:\n",
    "#         print(\"The model doesn't support feature importances.\")\n",
    "#         return None\n",
    "\n",
    "# def create_prediction_function(model):\n",
    "#     def predict_flood_risk(data_point):\n",
    "#         \"\"\"\n",
    "#         Predict flood risk for a single data point.\n",
    "        \n",
    "#         Parameters:\n",
    "#         - data_point: Dictionary with all required features\n",
    "        \n",
    "#         Returns:\n",
    "#         - Flood probability and binary prediction\n",
    "#         \"\"\"\n",
    "#         # Convert to DataFrame\n",
    "#         df = pd.DataFrame([data_point])\n",
    "        \n",
    "#         # Make prediction\n",
    "#         proba = model.predict_proba(df)[0, 1]\n",
    "#         prediction = 1 if proba >= 0.5 else 0\n",
    "        \n",
    "#         risk_level = \"Low\" if proba < 0.3 else \"Medium\" if proba < 0.7 else \"High\"\n",
    "        \n",
    "#         return {\n",
    "#             \"probability\": proba,\n",
    "#             \"prediction\": prediction,\n",
    "#             \"risk_level\": risk_level\n",
    "#         }\n",
    "    \n",
    "#     return predict_flood_risk\n",
    "\n",
    "# # Main execution\n",
    "# def main():\n",
    "#     # Load and explore data\n",
    "#     print(\"Loading and exploring data...\")\n",
    "#     perform_eda(data)\n",
    "    \n",
    "#     # Preprocess data\n",
    "#     print(\"\\nPreprocessing data...\")\n",
    "#     X_train, X_test, y_train, y_test, preprocessor, numerical_cols, categorical_cols = preprocess_data(data)\n",
    "    \n",
    "#     # Build models\n",
    "#     print(\"\\nBuilding models...\")\n",
    "#     models = build_models(X_train, y_train, preprocessor)\n",
    "    \n",
    "#     # Evaluate models\n",
    "#     print(\"\\nEvaluating models...\")\n",
    "#     results = evaluate_models(models, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "#     # Find the best model based on ROC-AUC\n",
    "#     best_model_name = max(results, key=lambda k: results[k]['roc_auc'])\n",
    "#     print(f\"\\nBest model: {best_model_name} with ROC-AUC: {results[best_model_name]['roc_auc']:.4f}\")\n",
    "    \n",
    "#     # Fine-tune the best model\n",
    "#     best_model = fine_tune_best_model(best_model_name, models, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "#     # Get feature importance\n",
    "#     top_features = feature_importance(best_model, numerical_cols, categorical_cols)\n",
    "#     if top_features:\n",
    "#         print(\"\\nTop 10 features:\")\n",
    "#         for feature, importance in top_features:\n",
    "#             print(f\"{feature}: {importance:.4f}\")\n",
    "    \n",
    "#     # Create prediction function\n",
    "#     predict_flood_risk = create_prediction_function(best_model)\n",
    "    \n",
    "#     # Example prediction\n",
    "#     example = {\n",
    "#         'region': 'Region_A',\n",
    "#         'rainfall_24h': 85.0,\n",
    "#         'rainfall_72h': 170.0,\n",
    "#         'river_level': 2.8,\n",
    "#         'soil_moisture': 75.0,\n",
    "#         'slope_gradient': 3.5,\n",
    "#         'vegetation_cover': 60.0,\n",
    "#         'urbanization': 70.0,\n",
    "#         'temperature': 22.0,\n",
    "#         'snowmelt': 5.0,\n",
    "#         'reservoir_level': 85.0,\n",
    "#         'previous_floods': 0,\n",
    "#         'year': 2023,\n",
    "#         'month': 6,\n",
    "#         'day': 15\n",
    "#     }\n",
    "    \n",
    "#     prediction = predict_flood_risk(example)\n",
    "#     print(\"\\nExample prediction:\")\n",
    "#     print(f\"Flood probability: {prediction['probability']:.2f}\")\n",
    "#     print(f\"Prediction: {'Flood' if prediction['prediction'] == 1 else 'No Flood'}\")\n",
    "#     print(f\"Risk level: {prediction['risk_level']}\")\n",
    "    \n",
    "#     print(\"\\nFlood prediction model successfully built and saved as 'flood_prediction_model.pkl'\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       326\n",
      "           1       0.93      0.86      0.90       274\n",
      "\n",
      "    accuracy                           0.91       600\n",
      "   macro avg       0.91      0.90      0.91       600\n",
      "weighted avg       0.91      0.91      0.91       600\n",
      "\n",
      "\n",
      "Training Gradient Boosting...\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       326\n",
      "           1       0.91      0.86      0.89       274\n",
      "\n",
      "    accuracy                           0.90       600\n",
      "   macro avg       0.90      0.90      0.90       600\n",
      "weighted avg       0.90      0.90      0.90       600\n",
      "\n",
      "\n",
      "Training XGBoost...\n",
      "\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91       326\n",
      "           1       0.93      0.84      0.89       274\n",
      "\n",
      "    accuracy                           0.90       600\n",
      "   macro avg       0.90      0.90      0.90       600\n",
      "weighted avg       0.90      0.90      0.90       600\n",
      "\n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90       326\n",
      "           1       0.89      0.86      0.87       274\n",
      "\n",
      "    accuracy                           0.89       600\n",
      "   macro avg       0.89      0.88      0.88       600\n",
      "weighted avg       0.89      0.89      0.88       600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anant\\hck\\main\\cld\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:05:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\anant\\hck\\main\\cld\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:05:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\anant\\hck\\main\\cld\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:05:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\anant\\hck\\main\\cld\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:05:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\anant\\hck\\main\\cld\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:05:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\anant\\hck\\main\\cld\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:05:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model 'Gradient Boosting' saved as 'flood_prediction_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"flood_prediction_dataset.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Convert date to datetime\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Drop the 'region' column\n",
    "data = data.drop(columns=['region'])\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(data):\n",
    "    X = data.drop(['flood_occurred', 'flood_probability', 'date'], axis=1)\n",
    "    y = data['flood_occurred']\n",
    "    numerical_cols = X.columns.tolist()\n",
    "    preprocessor = ColumnTransformer(transformers=[('num', StandardScaler(), numerical_cols)])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test, preprocessor, numerical_cols\n",
    "\n",
    "X_train, X_test, y_train, y_test, preprocessor, numerical_cols = preprocess_data(data)\n",
    "\n",
    "# Build models\n",
    "def build_models():\n",
    "    models = {\n",
    "        'Random Forest': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        'Gradient Boosting': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "        ]),\n",
    "        'XGBoost': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "        ]),\n",
    "        'Logistic Regression': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "        ])\n",
    "    }\n",
    "    return models\n",
    "\n",
    "models = build_models()\n",
    "\n",
    "def evaluate_models(models, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        print(f\"\\n{name} Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        results[name] = model\n",
    "    return results\n",
    "\n",
    "results = evaluate_models(models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Save the best model\n",
    "best_model_name = max(results, key=lambda k: cross_val_score(results[k], X_train, y_train, cv=5, scoring='roc_auc').mean())\n",
    "joblib.dump(results[best_model_name], \"flood_prediction_model.pkl\")\n",
    "print(f\"Best model '{best_model_name}' saved as 'flood_prediction_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing data...\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest ROC-AUC: 0.9226\n",
      "\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting ROC-AUC: 0.9351\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost ROC-AUC: 0.9109\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression ROC-AUC: 0.9356\n",
      "\n",
      "Saving best model...\n",
      "Model saved as 'flood_prediction_model.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anant\\hck\\main\\cld\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:49:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"flood_prediction_dataset.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Convert date to datetime\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(data):\n",
    "    # Separate features and target\n",
    "    X = data.drop(['flood_occurred', 'flood_probability', 'date'], axis=1)\n",
    "    \n",
    "    # Remove unwanted features\n",
    "    X = X[['rainfall_24h', 'rainfall_72h', 'river_level', 'soil_moisture', 'reservoir_level', 'previous_floods', 'temperature']]\n",
    "    \n",
    "    y = data['flood_occurred']\n",
    "    \n",
    "    # Create preprocessing steps\n",
    "    numerical_transformer = StandardScaler()\n",
    "    \n",
    "    # Create column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numerical_transformer, X.columns)])\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor, X.columns\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    X_train, X_test, y_train, y_test, preprocessor, feature_cols = preprocess_data(data)\n",
    "    \n",
    "    # Build and evaluate models\n",
    "    models = {\n",
    "        'Random Forest': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        'Gradient Boosting': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "        ]),\n",
    "        'XGBoost': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "        ]),\n",
    "        'Logistic Regression': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    best_model = None\n",
    "    best_roc_auc = 0\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        print(f\"{name} ROC-AUC: {roc_auc:.4f}\")\n",
    "        \n",
    "        if roc_auc > best_roc_auc:\n",
    "            best_roc_auc = roc_auc\n",
    "            best_model = model\n",
    "    \n",
    "    print(\"\\nSaving best model...\")\n",
    "    joblib.dump(best_model, \"flood_prediction_model.pkl\")\n",
    "    print(\"Model saved as 'flood_prediction_model.pkl'\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
